{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import logging\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_specific_special_chars(text):\n",
    "    # Define the specific special characters we're looking for\n",
    "    specific_chars = 'äöüßÄÖÜαβγδεζηθικλμνξοπρστυφχψωΔ'\n",
    "    found_chars = set()\n",
    "    \n",
    "    for char in specific_chars:\n",
    "        if char in text:\n",
    "            found_chars.add(char)\n",
    "    \n",
    "    logging.info(\"\\n=== Specific Special Characters Found ===\")\n",
    "    if found_chars:\n",
    "        for char in sorted(found_chars):\n",
    "            char_name = unicodedata.name(char)\n",
    "            logging.info(f\"Found '{char}' - {char_name}\")\n",
    "    else:\n",
    "        logging.info(\"None of the specific special characters were found in the text\")\n",
    "    \n",
    "    return found_chars\n",
    "\n",
    "def analyze_text_tokens(text):\n",
    "    logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Print initial text info\n",
    "    logger.info(\"\\n=== Input Text Statistics ===\")\n",
    "    logger.info(f\"Input text length: {len(text)} characters\")\n",
    "    \n",
    "    pattern = r\"\"\"\n",
    "        (?:\n",
    "            \\w*[äöüßÄÖÜαβγδεζηθικλμνξοπρστυφχψωΔ]+\\w*  # Words with special chars\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all matches\n",
    "    initial_tokens = re.findall(pattern, text, re.VERBOSE)\n",
    "    \n",
    "    # Clean tokens\n",
    "    initial_tokens = [\n",
    "        token.strip(' \"\\'') \n",
    "        for token in initial_tokens \n",
    "        if not any(char.isdigit() for char in token)\n",
    "    ]\n",
    "    \n",
    "    # Create summary statistics\n",
    "    summary_stats = {\n",
    "        \"total_initial_tokens\": len(initial_tokens),\n",
    "        \"unique_tokens\": len(set(initial_tokens)),\n",
    "        \"special_chars\": set(),\n",
    "        \"special_char_count\": 0\n",
    "    }\n",
    "    \n",
    "    # Analyze special characters\n",
    "    for token in initial_tokens:\n",
    "        for char in token:\n",
    "            if not char.isascii():\n",
    "                summary_stats[\"special_char_count\"] += 1\n",
    "                char_name = unicodedata.name(char, 'UNKNOWN')\n",
    "                char_category = unicodedata.category(char)\n",
    "                summary_stats[\"special_chars\"].add((char, char_name, char_category))\n",
    "    \n",
    "    # Print comprehensive summary\n",
    "    logger.info(\"\\n=== Token Analysis Summary ===\")\n",
    "    logger.info(f\"Total tokens detected: {summary_stats['total_initial_tokens']}\")\n",
    "    logger.info(f\"Unique tokens: {summary_stats['unique_tokens']}\")\n",
    "    logger.info(f\"Total special characters found: {summary_stats['special_char_count']}\")\n",
    "    \n",
    "    if summary_stats[\"special_chars\"]:\n",
    "        logger.info(\"\\n=== Special Characters Details ===\")\n",
    "        for char, name, category in sorted(summary_stats[\"special_chars\"]):\n",
    "            logger.info(f\"Character: '{char}' - Unicode: {name}, Category: {category}\")\n",
    "    \n",
    "    # Analyze all tokens with special characters\n",
    "    token_counts = Counter(initial_tokens)\n",
    "    \n",
    "    if token_counts:\n",
    "        logger.info(\"\\n=== Tokens with Special Characters ===\")\n",
    "        for token, count in sorted(token_counts.items()):\n",
    "            if count > 1:\n",
    "                logger.info(f\"'{token}' appears {count} times\")\n",
    "            else:\n",
    "                logger.info(f\"'{token}' appears once\")\n",
    "    \n",
    "    return summary_stats[\"special_chars\"]\n",
    "\n",
    "def detect_character_set(text):\n",
    "    logging.info(\"\\n=== Character Set Analysis ===\")\n",
    "    text_without_digits = ''.join(char for char in text if not char.isdigit())\n",
    "    unique_chars = set(text_without_digits)\n",
    "    special_chars = [c for c in unique_chars if not c.isascii()]\n",
    "    \n",
    "    found_special_chars = set()\n",
    "    for char in sorted(special_chars):\n",
    "        try:\n",
    "            char_name = unicodedata.name(char)\n",
    "            char_category = unicodedata.category(char)\n",
    "            logging.info(f\"Character: '{char}' - Unicode Name: {char_name}, Category: {char_category}\")\n",
    "            found_special_chars.add((char, char_name, char_category))\n",
    "        except ValueError:\n",
    "            logging.info(f\"Character: '{char}' - Unknown Unicode name\")\n",
    "    \n",
    "    return found_special_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Specific Special Characters Found ===\n",
      "Found 'Ä' - LATIN CAPITAL LETTER A WITH DIAERESIS\n",
      "Found 'Ö' - LATIN CAPITAL LETTER O WITH DIAERESIS\n",
      "Found 'Ü' - LATIN CAPITAL LETTER U WITH DIAERESIS\n",
      "Found 'ß' - LATIN SMALL LETTER SHARP S\n",
      "Found 'ä' - LATIN SMALL LETTER A WITH DIAERESIS\n",
      "Found 'ö' - LATIN SMALL LETTER O WITH DIAERESIS\n",
      "Found 'ü' - LATIN SMALL LETTER U WITH DIAERESIS\n",
      "Found 'δ' - GREEK SMALL LETTER DELTA\n",
      "Found 'μ' - GREEK SMALL LETTER MU\n",
      "\n",
      "=== Character Set Analysis ===\n",
      "Character: '«' - Unicode Name: LEFT-POINTING DOUBLE ANGLE QUOTATION MARK, Category: Pi\n",
      "Character: '°' - Unicode Name: DEGREE SIGN, Category: So\n",
      "Character: '»' - Unicode Name: RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK, Category: Pf\n",
      "Character: 'Ä' - Unicode Name: LATIN CAPITAL LETTER A WITH DIAERESIS, Category: Lu\n",
      "Character: 'Ö' - Unicode Name: LATIN CAPITAL LETTER O WITH DIAERESIS, Category: Lu\n",
      "Character: 'Ü' - Unicode Name: LATIN CAPITAL LETTER U WITH DIAERESIS, Category: Lu\n",
      "Character: 'ß' - Unicode Name: LATIN SMALL LETTER SHARP S, Category: Ll\n",
      "Character: 'ä' - Unicode Name: LATIN SMALL LETTER A WITH DIAERESIS, Category: Ll\n",
      "Character: 'æ' - Unicode Name: LATIN SMALL LETTER AE, Category: Ll\n",
      "Character: 'é' - Unicode Name: LATIN SMALL LETTER E WITH ACUTE, Category: Ll\n",
      "Character: 'ö' - Unicode Name: LATIN SMALL LETTER O WITH DIAERESIS, Category: Ll\n",
      "Character: 'ü' - Unicode Name: LATIN SMALL LETTER U WITH DIAERESIS, Category: Ll\n",
      "Character: 'δ' - Unicode Name: GREEK SMALL LETTER DELTA, Category: Ll\n",
      "Character: 'μ' - Unicode Name: GREEK SMALL LETTER MU, Category: Ll\n",
      "Character: 'А' - Unicode Name: CYRILLIC CAPITAL LETTER A, Category: Lu\n",
      "Character: 'Е' - Unicode Name: CYRILLIC CAPITAL LETTER IE, Category: Lu\n",
      "Character: 'К' - Unicode Name: CYRILLIC CAPITAL LETTER KA, Category: Lu\n",
      "Character: 'Н' - Unicode Name: CYRILLIC CAPITAL LETTER EN, Category: Lu\n",
      "Character: 'О' - Unicode Name: CYRILLIC CAPITAL LETTER O, Category: Lu\n",
      "Character: 'Р' - Unicode Name: CYRILLIC CAPITAL LETTER ER, Category: Lu\n",
      "Character: 'С' - Unicode Name: CYRILLIC CAPITAL LETTER ES, Category: Lu\n",
      "Character: 'Т' - Unicode Name: CYRILLIC CAPITAL LETTER TE, Category: Lu\n",
      "Character: 'Ь' - Unicode Name: CYRILLIC CAPITAL LETTER SOFT SIGN, Category: Lu\n",
      "Character: 'а' - Unicode Name: CYRILLIC SMALL LETTER A, Category: Ll\n",
      "Character: 'в' - Unicode Name: CYRILLIC SMALL LETTER VE, Category: Ll\n",
      "Character: 'д' - Unicode Name: CYRILLIC SMALL LETTER DE, Category: Ll\n",
      "Character: 'е' - Unicode Name: CYRILLIC SMALL LETTER IE, Category: Ll\n",
      "Character: 'и' - Unicode Name: CYRILLIC SMALL LETTER I, Category: Ll\n",
      "Character: 'л' - Unicode Name: CYRILLIC SMALL LETTER EL, Category: Ll\n",
      "Character: 'м' - Unicode Name: CYRILLIC SMALL LETTER EM, Category: Ll\n",
      "Character: 'н' - Unicode Name: CYRILLIC SMALL LETTER EN, Category: Ll\n",
      "Character: 'о' - Unicode Name: CYRILLIC SMALL LETTER O, Category: Ll\n",
      "Character: 'п' - Unicode Name: CYRILLIC SMALL LETTER PE, Category: Ll\n",
      "Character: 'р' - Unicode Name: CYRILLIC SMALL LETTER ER, Category: Ll\n",
      "Character: 'с' - Unicode Name: CYRILLIC SMALL LETTER ES, Category: Ll\n",
      "Character: 'т' - Unicode Name: CYRILLIC SMALL LETTER TE, Category: Ll\n",
      "Character: '…' - Unicode Name: HORIZONTAL ELLIPSIS, Category: Po\n",
      "\n",
      "=== Input Text Statistics ===\n",
      "Input text length: 62635 characters\n",
      "\n",
      "=== Token Analysis Summary ===\n",
      "Total tokens detected: 289\n",
      "Unique tokens: 139\n",
      "Total special characters found: 300\n",
      "\n",
      "=== Special Characters Details ===\n",
      "Character: 'Ä' - Unicode: LATIN CAPITAL LETTER A WITH DIAERESIS, Category: Lu\n",
      "Character: 'Ö' - Unicode: LATIN CAPITAL LETTER O WITH DIAERESIS, Category: Lu\n",
      "Character: 'Ü' - Unicode: LATIN CAPITAL LETTER U WITH DIAERESIS, Category: Lu\n",
      "Character: 'ß' - Unicode: LATIN SMALL LETTER SHARP S, Category: Ll\n",
      "Character: 'ä' - Unicode: LATIN SMALL LETTER A WITH DIAERESIS, Category: Ll\n",
      "Character: 'ö' - Unicode: LATIN SMALL LETTER O WITH DIAERESIS, Category: Ll\n",
      "Character: 'ü' - Unicode: LATIN SMALL LETTER U WITH DIAERESIS, Category: Ll\n",
      "Character: 'δ' - Unicode: GREEK SMALL LETTER DELTA, Category: Ll\n",
      "\n",
      "=== Tokens with Special Characters ===\n",
      "'Abklärung' appears once\n",
      "'Aktivität' appears 5 times\n",
      "'Aktivitätsparameter' appears once\n",
      "'Altersabhängig' appears once\n",
      "'Aminobuttersäure' appears once\n",
      "'Aminosäuren' appears once\n",
      "'Antikörper' appears 4 times\n",
      "'Antikörperbefund' appears once\n",
      "'Antikörpern' appears once\n",
      "'Antinukleäre' appears once\n",
      "'Anämiediagnostik' appears once\n",
      "'Asparaginsäure' appears once\n",
      "'Atopieabklärung' appears once\n",
      "'Auftragsschlüssel' appears 2 times\n",
      "'Ausschließlich' appears once\n",
      "'Autoantikörper' appears 3 times\n",
      "'Basenüberschuß' appears once\n",
      "'Beeinträchtigung' appears once\n",
      "'Berücksichtigung' appears once\n",
      "'Bäcker' appears once\n",
      "'Einschätzung' appears once\n",
      "'Eiweiß' appears 6 times\n",
      "'Eiweißkorr' appears once\n",
      "'Entzündung' appears 2 times\n",
      "'Entzündungsreaktion' appears once\n",
      "'Erfaßt' appears once\n",
      "'Ergänzende' appears once\n",
      "'Erhöhtes' appears once\n",
      "'Fettsäuren' appears once\n",
      "'Folliklulär' appears once\n",
      "'Folsäure' appears 6 times\n",
      "'Frühbande' appears once\n",
      "'Frühmarker' appears once\n",
      "'Gallensäuren' appears once\n",
      "'Gemaß' appears once\n",
      "'Gemäß' appears once\n",
      "'Gesamteiweiß' appears once\n",
      "'Glomuläre' appears once\n",
      "'Glutaminsäure' appears once\n",
      "'Große' appears once\n",
      "'Großes' appears 7 times\n",
      "'Harnsäure' appears 11 times\n",
      "'Hypercholesterinämie' appears 2 times\n",
      "'Hämatokrit' appears 23 times\n",
      "'Hämatologie' appears 9 times\n",
      "'Hämochromatose' appears once\n",
      "'Hämoglobin' appears 27 times\n",
      "'Hämostaseologie' appears once\n",
      "'Hämsto' appears once\n",
      "'Ketonkörpernachw' appears once\n",
      "'Ketonkδrper' appears once\n",
      "'Körperoberfläche' appears once\n",
      "'Lützowstr' appears once\n",
      "'Lützowstraße' appears once\n",
      "'Methylmaionsäure' appears once\n",
      "'Mikronährstoffe' appears once\n",
      "'Müller' appears once\n",
      "'Münster' appears 2 times\n",
      "'Nüchtern' appears once\n",
      "'Plättchen' appears once\n",
      "'Primärinfektion' appears once\n",
      "'Präventiv' appears once\n",
      "'Präventivmedizinischer' appears 5 times\n",
      "'Prülgegenstände' appears once\n",
      "'Reserveatmungskapazität' appears once\n",
      "'Röhrborn' appears 2 times\n",
      "'Röhrchen' appears 2 times\n",
      "'Schilddrüse' appears 3 times\n",
      "'Schädigung' appears once\n",
      "'Sekundäres' appears once\n",
      "'Spezialröhrchen' appears once\n",
      "'Spezifität' appears once\n",
      "'Stuhlröhrchen' appears once\n",
      "'Störung' appears once\n",
      "'Sättigu' appears once\n",
      "'Sättigung' appears once\n",
      "'Thalassämie' appears once\n",
      "'Transferrinsättigung' appears 2 times\n",
      "'Unauffällige' appears once\n",
      "'Universitätsmedizin' appears once\n",
      "'Vergrößerung' appears once\n",
      "'Zelluläres' appears 2 times\n",
      "'abgeschätzt' appears once\n",
      "'altersabhängig' appears once\n",
      "'auszuschließen' appears once\n",
      "'durchgeführt' appears once\n",
      "'einschränkung' appears once\n",
      "'entzündlichen' appears once\n",
      "'ergänzende' appears once\n",
      "'erhöht' appears 5 times\n",
      "'erhöhte' appears 4 times\n",
      "'erhöhtem' appears once\n",
      "'erhöhten' appears once\n",
      "'erhöhter' appears once\n",
      "'erhöhtes' appears 2 times\n",
      "'frühem' appears once\n",
      "'fördern' appears once\n",
      "'führen' appears once\n",
      "'für' appears 13 times\n",
      "'gemäß' appears 2 times\n",
      "'geschätzte' appears once\n",
      "'gestört' appears once\n",
      "'hypophysären' appears once\n",
      "'hämolytisch' appears once\n",
      "'indikationsabhängig' appears once\n",
      "'kapillär' appears once\n",
      "'kardiovaskuläres' appears 10 times\n",
      "'können' appears 4 times\n",
      "'lipämisch' appears once\n",
      "'lös' appears once\n",
      "'lösl' appears once\n",
      "'männlich' appears 2 times\n",
      "'mässige' appears once\n",
      "'möglich' appears 4 times\n",
      "'nü' appears once\n",
      "'primären' appears once\n",
      "'proentzündlicher' appears once\n",
      "'schließen' appears 2 times\n",
      "'sekundären' appears once\n",
      "'unauffällige' appears once\n",
      "'ursächliche' appears once\n",
      "'venös' appears once\n",
      "'verändernder' appears once\n",
      "'vollständig' appears once\n",
      "'wäre' appears once\n",
      "'wünschenswerter' appears once\n",
      "'zurückliegende' appears 3 times\n",
      "'zusätzlich' appears once\n",
      "'zuverlässig' appears once\n",
      "'zuverlässigere' appears once\n",
      "'zyklusabhängi' appears once\n",
      "'zyklusabhängig' appears once\n",
      "'Ärztin' appears 2 times\n",
      "'Östradiol' appears 4 times\n",
      "'Über' appears once\n",
      "'Überprüfung' appears 2 times\n",
      "'Überwiegen' appears once\n",
      "'über' appears once\n",
      "'überschießende' appears once\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "    \n",
    "    # Specify your input file path\n",
    "    file_path = r'd:\\OneDrive - Green Energy\\Desktop\\HF_NER_Med\\data\\kitchen\\reports\\ocr_texts.txt'  # Replace with your file path\n",
    "    \n",
    "    # Read the text file\n",
    "    text = read_text_file(file_path)\n",
    "    \n",
    "    if text:\n",
    "        # Check for specific special characters first\n",
    "        specific_chars_found = check_specific_special_chars(text)\n",
    "        \n",
    "        # Then analyze the character set\n",
    "        char_set_results = detect_character_set(text)\n",
    "        \n",
    "        # Finally process the tokens\n",
    "        token_results = analyze_text_tokens(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
